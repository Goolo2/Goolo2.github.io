<!DOCTYPE HTML>
<html lang="zh-CN">


<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="损失函数CrossEntropyLoss详解, Goolo">
    <meta name="description" content="损失函数CrossEntropyLoss详解1.nn与nn.functional的区别
torch.nn下的Conv1d
import torch.nn.functional as F
class Conv1d(_ConvNd):
    ">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>损失函数CrossEntropyLoss详解 | Goolo</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>

<header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Goolo</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>首页</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>标签</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>分类</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>归档</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>关于</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/friends" class="waves-effect waves-light">
            
            <i class="fa fa-address-book"></i>
            
            <span>友情链接</span>
        </a>
    </li>
    
    <li>
        <a href="#searchModal" class="modal-trigger waves-effect waves-light">
            <i id="searchIcon" class="fa fa-search" title="搜索"></i>
        </a>
    </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Goolo</div>
        <div class="logo-desc">
            
            种豆子和相思或许都得瓜。
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                首页
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                标签
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                分类
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                归档
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                关于
            </a>
        </li>
        
        <li>
            <a href="/friends" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-address-book"></i>
                
                友情链接
            </a>
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fa fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>


<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/18.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        损失函数CrossEntropyLoss详解
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 20px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/概念/" target="_blank">
                                <span class="chip bg-color">概念</span>
                            </a>
                        
                            <a href="/tags/基础/" target="_blank">
                                <span class="chip bg-color">基础</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/DL/" class="post-category" target="_blank">
                                DL
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2019-08-12
                </div>

                
                    
                    <div class="info-break-policy">
                        <i class="fa fa-file-word-o fa-fw"></i>文章字数:&nbsp;&nbsp;
                        4.3k
                    </div>
                    

                    
                    <div class="info-break-policy">
                        <i class="fa fa-clock-o fa-fw"></i>阅读时长:&nbsp;&nbsp;
                        17 分
                    </div>
                    
                
				
				
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="fa fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="损失函数CrossEntropyLoss详解"><a href="#损失函数CrossEntropyLoss详解" class="headerlink" title="损失函数CrossEntropyLoss详解"></a>损失函数CrossEntropyLoss详解</h1><h2 id="1-nn与nn-functional的区别"><a href="#1-nn与nn-functional的区别" class="headerlink" title="1.nn与nn.functional的区别"></a>1.nn与nn.functional的区别</h2><ol>
<li><p><strong>torch.nn下的Conv1d</strong></p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">class</span> <span class="token class-name">Conv1d</span><span class="token punctuation">(</span>_ConvNd<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
                 padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        kernel_size <span class="token operator">=</span> _single<span class="token punctuation">(</span>kernel_size<span class="token punctuation">)</span>
        stride <span class="token operator">=</span> _single<span class="token punctuation">(</span>stride<span class="token punctuation">)</span>
        padding <span class="token operator">=</span> _single<span class="token punctuation">(</span>padding<span class="token punctuation">)</span>
        dilation <span class="token operator">=</span> _single<span class="token punctuation">(</span>dilation<span class="token punctuation">)</span>
        super<span class="token punctuation">(</span>Conv1d<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>
            in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> padding<span class="token punctuation">,</span> dilation<span class="token punctuation">,</span>
            <span class="token boolean">False</span><span class="token punctuation">,</span> _single<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> groups<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>

        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> F<span class="token punctuation">.</span>conv1d<span class="token punctuation">(</span>input<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> self<span class="token punctuation">.</span>stride<span class="token punctuation">,</span>
                            self<span class="token punctuation">.</span>padding<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dilation<span class="token punctuation">,</span> self<span class="token punctuation">.</span>groups<span class="token punctuation">)</span></code></pre>
</li>
<li><p><strong>torch.nn.functional下的conv1d:</strong></p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">conv1d</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token operator">=</span>None<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
           groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> input <span class="token keyword">is</span> <span class="token operator">not</span> None <span class="token operator">and</span> input<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">3</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Expected 3D tensor as input, got {}D tensor instead."</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>input<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        f <span class="token operator">=</span> ConvNd<span class="token punctuation">(</span>_single<span class="token punctuation">(</span>stride<span class="token punctuation">)</span><span class="token punctuation">,</span> _single<span class="token punctuation">(</span>padding<span class="token punctuation">)</span><span class="token punctuation">,</span> _single<span class="token punctuation">(</span>dilation<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
                   _single<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> groups<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>benchmark<span class="token punctuation">,</span>
                   torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>enabled<span class="token punctuation">)</span>
        <span class="token keyword">return</span> f<span class="token punctuation">(</span>input<span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token punctuation">)</span></code></pre>
</li>
<li><p><strong>对比</strong></p>
<p>可以看到torch.nn下的Conv1d类在forward时调用了nn.functional下的conv1d。</p>
<p>这么设计是有其原因的。如果我们只保留nn.functional下的函数的话，在训练或者使用时，我们就要手动去维护weight, bias, stride这些中间量的值，这显然是给用户带来了不便。而如果我们只保留nn下的类的话，其实就牺牲了一部分灵活性，因为做一些简单的计算都需要创造一个类，这也与PyTorch的风格不符。</p>
<ul>
<li><p><strong>调用方式对比</strong></p>
<p><code>nn.Xxx</code> 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。</p>
<pre class=" language-python"><code class="language-python">inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">244</span><span class="token punctuation">,</span> <span class="token number">244</span><span class="token punctuation">)</span>
conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
out <span class="token operator">=</span> conv<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span></code></pre>
<p><code>nn.functional.xxx</code>同时传入输入数据和weight, bias等其他参数 。</p>
<pre class=" language-python"><code class="language-python">weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>
bias <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span> 
out <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre>
</li>
<li><p><strong>nn.Xxx继承于nn.Module， 能够很好的与nn.Sequential结合使用， 而nn.functional.xxx无法与nn.Sequential结合使用。</strong></p>
<pre class=" language-python"><code class="language-python">fm_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_features<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
  <span class="token punctuation">)</span></code></pre>
</li>
</ul>
</li>
</ol>
<ol start="4">
<li><p><strong>举例</strong></p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>F<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x</code></pre>
</li>
</ol>
<p>以一个最简单的三层网络为例。需要维持状态的，主要是三个线性变换，所以在构造Module是，定义了三个nn.Linear对象，而在计算时，relu,dropout之类不需要保存状态的可以直接使用。</p>
<p>问题回答：</p>
<ul>
<li><p>什么是维持状态？</p>
<p>拿线性变化举例吧，其中的权重需要不停的更新，这就是需要维持的状态，而以激活函数来说，一般来说都是可以直接使用的，只要给定相应的输入，那么使用后就有确定的输出。</p>
</li>
<li><p>像是relu这种东西，明摆着就是不用更新状态的，为啥nn里面还要给实现一次？</p>
<p>还有一种利用Sequential进行模型搭建的方式，里面要求每个参数都是nn.Module，这时候就可以派上用场了。</p>
<p>   nn.ModuleList()也会用到</p>
</li>
</ul>
<p>作者：蒲嘉宸<br>链接：<a href="https://www.zhihu.com/question/66782101/answer/246341271" target="_blank" rel="noopener">https://www.zhihu.com/question/66782101/answer/246341271</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<h2 id="2-Softmax详解"><a href="#2-Softmax详解" class="headerlink" title="2. Softmax详解"></a>2. Softmax详解</h2><p>参考自<a href="https://www.cnblogs.com/marsggbo/p/10401215.html" target="_blank" rel="noopener">https://www.cnblogs.com/marsggbo/p/10401215.html</a></p>
<h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><p>Softmax函数的输入是N维的随机真值向量，输出是另一个N维的真值向量，<br>且值的范围是(0,1)(0,1)，和为1.0。即映射</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565525600336.png" alt="1565525600336"></p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565525607664.png" alt="1565525607664"></p>
<p>其中每一个元素的公式为： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565525660131.png" alt="1565525660131"></p>
<p>显然SjSj总是正的~(因为指数)；因为所有的SjSj的和为1，所以有Sj&lt;1Sj&lt;1，因此它的范围是(0,1)(0,1)。例如，一个含有三个元素的向量[1.0,2.0,3.0][1.0,2.0,3.0]被转化为[0.09,0.24,0.67][0.09,0.24,0.67]。<br>转化后的元素与原始的对应的元素位置上保持一致，且和为1。我们将原始的向量拉伸为[1.0,2.0,5.0][1.0,2.0,5.0]，得到变换后的[0.02,0.05,0.93][0.02,0.05,0.93]，同样具有前面的性质。</p>
<p>注意此时因为最后一个元素(5.0)距离前面两个元素(1.0和2.0)较远，因此它的输出的softmax值占据了和1.0的大部分(0.93)。softmax并不是只选择一个最大元素，而是将向量分解为整个(1.0)的几部分，最大的输入元素得到一个比例较大的部分，但其他元素各自也获得对应的部分。</p>
<h3 id="概率解释"><a href="#概率解释" class="headerlink" title="概率解释"></a>概率解释</h3><p>softmax的性质(所有输出的值范围是(0,1)且和为1.0)使其在机器学习的概率解释中广泛使用。尤其是在多类别分类任务中，我们总是给输出结果对应的类别附上一个概率，即如果我们的输出类别有N种，我们就输出一个N维的概率向量且和为1.0。每一维的值对应一种类别的概率。我们可以将softmax解释如下： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565525796724.png" alt="1565525796724"></p>
<p>其中，yy是输出的N个类别中的某个(取值为1…N1…N)。aa是任意一个N维向量。最常见的例子是多类别的逻辑斯谛回归，输入的向量xx乘以一个权重矩阵W，且该结果输入softmax函数以产生概率。我们在后面会探讨这个结构。事实证明，从概率的角度来看，softmax对于模型参数的最大似然估计是最优的。 不过，这超出了本文的范围。有关更多详细信息，请参阅“深度学习”一书的第5章(链接：<a href="http://www.deeplearningbook.org)。" target="_blank" rel="noopener">www.deeplearningbook.org)。</a></p>
<h3 id="Softmax函数的导数"><a href="#Softmax函数的导数" class="headerlink" title="Softmax函数的导数"></a>Softmax函数的导数</h3><p>首先，明确我们计算的偏导数是什么</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565533120501.png" alt="1565533120501"></p>
<p>这是==第i个输出关于第j个输入的偏导数==。我们使用一个更简洁的式子来表示：$D_jS_i$</p>
<p>因为softmax函数是一个$\mathbb{R}^{N}-&gt;\mathbb{R}^N$的函数，所以我们计算得到的导数是一个雅可比矩阵：（<strong>列数由输入决定，行书由输出决定</strong>）</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565533408247.png" alt="1565533408247"></p>
<p>在机器学习的文献中，常常用术语梯度来表示通常所说的导数。严格来说，梯度只是为标量函数来定义的，例如机器学习中的损失函数；对于像softmax这样的向量函数，说是“梯度”是不准确的；==雅可比是一个向量函数的全部的导数，大多数情况下我们会说“导数”。==</p>
<p>对任意的ii和jj，让我们来计算$D_jS_i$</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565573031346.png" alt="1565573031346"></p>
<p>我们将使用链式法则来计算导数，即对于$f(x)=g(x)/h(x)$</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565573080090.png" alt="1565573080090"></p>
<p>在我们的情况下，有： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565573095898.png" alt="1565573095898"></p>
<p>注意对于$h_i$，无论求其关于哪个$a_j$的导数，结果都是$e^{a_j}$，但是对于$g_i$就不同了，$g_i$关于$a_j$的导数是</p>
<p>$e^{a_j}$，当且仅当$i=j$，否则结果为0。</p>
<p>让我们回到$D_jS_i$；我们先考虑$i=j$的情况。根据链式法则我们有： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565573652017.png" alt="1565573652017"></p>
<p>简单起见，我们使用$\sum_{}$表示$\sum_{n=1}^Ne^{a_k}$，继续简化如下</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565573837347.png" alt="1565573837347"></p>
<p>类似的，考虑$i\not=j$的情况</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565573900202.png" alt="1565573900202"></p>
<p>总结如下： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565573914433.png" alt="1565573914433"></p>
<p>在文献中我们常常会见到各种各样的”浓缩的”公式，一个常见的例子是使用==克罗内克函数==： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565573945730.png" alt="1565573945730"></p>
<p>于是我们有：</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565573969344.png" alt="1565573969344"></p>
<p><strong>在文献中也有一些其它的表述：</strong></p>
<ul>
<li>在雅可比矩阵中使用单位矩阵$I$来替换$\delta$，$I$使用元素的矩阵形式表示了δ。</li>
<li>使用”1”作为函数名而不是克罗内克δ，如下所示：$D_jS_i=S_i(1(i=j)-S_j)$。这里1($i=j$)意味着当$i=j$时值为1，否则为0。</li>
</ul>
<h3 id="数值稳定性（规范化）"><a href="#数值稳定性（规范化）" class="headerlink" title="数值稳定性（规范化）"></a>数值稳定性（规范化）</h3><p>对于一个给定的向量，利用Python来计算softmax的简单方法</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Compute the softmax of vector x."""</span>
    exps <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    <span class="token keyword">return</span> exps <span class="token operator">/</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>exps<span class="token punctuation">)</span></code></pre>
<pre><code>In [146]: softmax([1, 2, 3])
Out[146]: array([ 0.09003057, 0.24472847,  0.66524096])</code></pre><p>然而当我们使用该函数计算较大的值时(或者大的负数时)，会出现一个问题：</p>
<pre><code>In [148]: softmax([1000, 2000, 3000])
Out[148]: array([ nan,  nan,  nan])</code></pre><p>Numpy使用的浮点数的数值范围是有限的。对于float64，最大可表示数字的大小为1030810308。 </p>
<p>因此我们需要通过规范输入使其输入不要太大或者太小，通过观察我们可以使用任意的常量C，如下所示： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565579046188.png" alt="1565579046188"></p>
<p>然后将这个变量转换到指数上： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565579057919.png" alt="1565579057919"></p>
<p>因为C是一个随机的常量，所以我们可以写为： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565579081353.png" alt="1565579081353"></p>
<p>D也是一个任意常量。对任意D，这个公式等价于前面的式子，这让我们能够更好的进行计算。对于D，一个比较好的选择是所有输入的最大值的负数： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565579113195.png" alt="1565579113195"></p>
<p>假定输入本身彼此相差不大，这会使输入转换到接近于0的范围。最重要的是，它将所有的输入转换为负数(除最大值外，最大值变为0)。很大的负指数结果会趋于0而不是无穷，这就让我们很好的避免了出现NaN的结果。</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">stablesoftmax</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Compute the softmax of vector x in a numerically
    stable way."""</span>
    shiftx <span class="token operator">=</span> x <span class="token operator">-</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    exps <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>shiftx<span class="token punctuation">)</span>
    <span class="token keyword">return</span> exps <span class="token operator">/</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>exps<span class="token punctuation">)</span></code></pre>
<pre><code>In [150]: stablesoftmax([1000, 2000, 3000])
Out[150]: array([ 0.,  0.,  1.])</code></pre><h3 id="机器学习中的softmax层及其导数"><a href="#机器学习中的softmax层及其导数" class="headerlink" title="机器学习中的softmax层及其导数"></a>机器学习中的softmax层及其导数</h3><p>softmax常用于机器学习中，特别是逻辑斯特回归：softmax层，其中我们将softmax应用于全连接层(矩阵乘法)的输出，如图所示。 </p>
<p><img src="https://img-blog.csdn.net/20180426095118859?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L2Nhc3NpZVB5dGhvbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们如何计算这个“softmax层”的导数(先进行全连接矩阵乘法，然后是softmax)？当然是使用链式规则！</p>
<p>在我们开始之前的一个重要的观点：<strong>你可能会认为x是计算其导数的自然变量(natural variable)</strong>。但事实并非如此。实际上，在机器学习中，我们通常希望找到最佳的权重矩阵W，因此我们希望用梯度下降的每一步来更新权重。<strong>因此，实际上我们将计算该层的关于W的导数。</strong></p>
<p>我们首先将这个图改写为向量函数的组合。首先我们定义矩阵乘法<code>g(W)</code>，即映射：$\mathbb{R}^{NT}-&gt;\mathbb{R}^T$。因为输入(矩阵W)$N \times T$个元素，输出有T个元素。</p>
<p>接下来我们来考虑softmax，如果我们定义logits的向量是λ，我们有：$\mathbb{R}^{T}-&gt;\mathbb{R}^T$。总体来说，我们有：</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565579770681.png" alt="1565579770681"></p>
<p>==这里的g(W)是指使W权重矩阵变为logits层的函数==</p>
<p>==S(W)是Softmax函数，使logtis层变为prob层==</p>
<p>==因此整个图可以表示为P(W)=S(g(W))==</p>
<p>使用多变量的链式法则，得到P(W)的雅可比矩阵： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565579849106.png" alt="1565579849106"></p>
<blockquote>
<p><strong>Sog是复合函数的一种记法</strong></p>
<p>复合函数的链式求导举例：<br>$$<br>h(a)=f[g(a)]<br>$$<br>则<br>$$<br>h’(a)=f’[g(x)]\cdot g’(x)<br>$$<br>如</p>
<p>$f(x)=3x,g(x)=3x+3,求h=g(f(x))的导数$</p>
<p>解：</p>
<p>$令t=f(x)=3x$</p>
<p>$那么h’(x)=g’(t)f’(x)=(3t+3)’(3x)’=3 \times 3=9$</p>
</blockquote>
<p>这里注意正确计算相应的索引。因为g(W)：$\mathbb{R}^{NT}-&gt;\mathbb{R}^T$</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565581084832.png" alt="1565581084832"></p>
<p>（<strong>雅克比矩阵列数由输入决定，行数由输出决定</strong>）</p>
<ul>
<li><p><u><strong>先让我们回顾一下W</strong></u><br>$$<br>\left[<br>\begin{matrix}<br> W_{11}      &amp;  W_{12}       &amp; \cdots &amp;  W_{1N}      \\<br>  W_{21}      &amp; W_{22}      &amp; \cdots &amp; W_{2N}    \\<br> \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br> W_{T1}     &amp; W_{T2}     &amp; \cdots &amp; W_{TN}      \<br>\end{matrix}<br>\right]<br>$$<br>我们可以看到输入是T×N的矩阵。</p>
<p>而在上面的雅克比矩阵Dg中，在某种意义上，<strong>权重矩阵W被“线性化”为长度为NT的向量</strong>。 如果您熟悉多维数组的内存布局，应该很容易理解它是如何完成的。 （按照行主次序对其进行线性化处理，第一行是连续的，接着是第二行，等等。$W_{ij}$在雅可比矩阵中的列号是<code>(i-1)N</code>）</p>
</li>
<li><p><u><strong>再让我们回顾一下logits层</strong></u><br>$$<br>\left[<br>\begin{matrix}<br> g_1          \<br> g_2           \<br> \vdots \<br> g_T           \<br>\end{matrix}<br>\right]<br>$$</p>
</li>
</ul>
<p>  其中</p>
<p>  <img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565593006137.png" alt="1565593006137"></p>
<p>结合前面$D_jS_i$这种写法的定义（就是求偏导），我们可以类比得到<br>$$<br>D_1g_1=x_1\<br>D_2g_1=x_2\<br>\cdots\<br>D_Ng_1=x_N\<br>D_{N+1}g_1=0\<br>\cdots\<br>D_{NT}g_1=0<br>$$<br>我们使用同样的策略来计算g2⋯gT，我们可以得到雅可比矩阵： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565593332504.png" alt="1565593332504"></p>
<p>从另一个角度来这个问题，如果我们将W的索引分解为i和j，可以得到</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565593355749.png" alt="1565593355749"></p>
<p>最后，为了计算softmax层的完整的雅可比矩阵，我们只需要计算DSDS和DgDg间的乘积。注意P(W)：$\mathbb{R}^{NT}-&gt;\mathbb{R}^T$，因此雅可比矩阵的维度可以确定。因此DS是T×T，Dg是T×NT的，它们的乘积DP是T×NT的。 </p>
<p>在文献中，你会看到softmax层的导数大大减少了。因为涉及的两个函数很简单而且很常用。 如果我们仔细计算DS的行和Dg的列之间的乘积：</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565593617650.png" alt="1565593617650"></p>
<p>Dg大多数为0，所以最终的结果很简单，仅当i=k时$D_{ij}g_k$不为0；然后它等于$x_j$。因此： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565593734795.png" alt="1565593734795"></p>
<p>[克罗内克函数 $t=i$时 $\delta_{ti}=1$]</p>
<p>因此完全可以在没有实际雅可比矩阵乘法的情况下计算softmax层的导数; 这很好，因为矩阵乘法很耗时！由于全连接层的雅可比矩阵是稀疏的，我们可以避免大多数计算。</p>
<h2 id="3-Softmax和交叉熵损失"><a href="#3-Softmax和交叉熵损失" class="headerlink" title="3. Softmax和交叉熵损失"></a>3. Softmax和交叉熵损失</h2><p>我们刚刚看到softmax函数如何用作机器学习网络的一部分，以及如何使用多元链式规则计算它的导数。当我们处理这个问题的时候，经常看到损失函数和softmax一起使用来训练网络：交叉熵。</p>
<p>交叉熵有一个有趣的概率和信息理论解释，但在这里我只关注其使用机制。对于两个离散概率分布p和q，交叉熵函数定义为： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565593961323.png" alt="1565593961323"></p>
<p>其中kk遍历分布定义的随机变量的所有的可能的值。具体而言，在我们的例子中有T个输出类别，所以k取值从1到T。<br>如果我们从softmax的输出P(一个概率分布)来考量。其它的概率分布是”正确的”类别输出，通常定义为Y，是一个大小为T的<strong>one-hot编码</strong>的向量，其中一个元素为1.0(该元素表示正确的类别)，其它为0。让我们重新定义该情况下的交叉熵公式:</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565594038254.png" alt="1565594038254"></p>
<p>其中k遍历所有的输出类别，<strong>P(k)是模型预测的类别的概率。Y(k)是数据集提供的真正的类别概率</strong>。我们定义唯一的Y(k)=1.0的索引为y，因此对所有的k≠y，都有Y(k)=0，于是交叉熵公式可以简化为:</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565594081511.png" alt="1565594081511"></p>
<p>实际上，我们把y当作一个常量，仅使用P来表示这个函数。进一步地，因为在我们的例子中P是一个向量，我们可以将P(y)表示为P的 第y个元素，即Py： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565594103717.png" alt="1565594103717"></p>
<p>xent的雅可比矩阵是1×T的矩阵(一个行向量)。因为输出是一个标量且我们有T个输出(向量P有T个元素)： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565594230530.png" alt="1565594230530"></p>
<p>现在回顾下P可以表示为输入为权值的函数：P(W)=S(g(W))。所以我们有另一个函数表示： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565594313406.png" alt="1565594313406"></p>
<p><strong>【注意上式最左边的表示有歧义，以最右边的表示来理解】</strong></p>
<p>我们可以再次使用多元链式法则来计算xent关于W的梯度： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565594436636.png" alt="1565594436636"></p>
<p>我们来检查一下雅可比行矩阵的维数。我们已经计算过了DP(W)，它是T×NT的。Dxent(P(W))是<strong>1×T</strong>的，所以得到的 雅可比矩阵Dxent(W)是<strong>1×NT</strong>的。这是有意义的，因为整个网络有一个输出(交叉熵损失，是一个标量)和NT个输入(权重)。 </p>
<p>同样的，有一个简单的方式来找到Dxent(W)的简单公式，因为矩阵乘法中的许多元素最终会被消除。注意到xent(P)只依赖于P的<br>第y个元素。因此，在雅可比矩阵中，只有$D_yxent$是非0的： </p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565594779407.png" alt="1565594779407"></p>
<p>其中，$D_yxent=-1/p_y$。回到整个的雅可比矩阵Dxent(W)，使Dxent(P)乘以D(P(W))的每一列，得到结果的行向量的每一个元素。回顾用行向量表示的按行优先的“线性化”的整个权重矩阵W。清晰起见，我们将使用i和j来索引它(DijDij)表示行向量的中的第iN+j个元素)：</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565598146364.png" alt="1565598146364"></p>
<p>因为在$D_kxent(P)$中只有第y个元素是非0的，所以我们可以得到下式：</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565598211936.png" alt="1565598211936"></p>
<p>根据我们的定义，$P_y=S_y$，所以可得：</p>
<p><img src="D:%5C%E7%BC%96%E7%A8%8B%5CDataCamp%5Cdatanote%5Cpic%5C1565598241672.png" alt="1565598241672"></p>
<p>即使最终的结果很简洁清楚，但是我们不一定非要这样做。公式Dijxent(W)Dijxent(W)可能最终成为一个和的形式(或者某些和的和)。关于雅可比矩阵的这些技巧可能并没有太大意义，因为计算机可以完成所有的工作。我们需要做的就是计算出单个的雅矩阵，这通常毕竟容易，因为它们是更简单的非复合函数。这技术体现了多元链式法则的美妙和实用性。</p>

            </div>
            <hr/>

            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>微信里点“发现”->“扫一扫”二维码便可查看分享。</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            

    <div class="reprint" id="reprint-statement">
        <p class="reprint-tip">
            <i class="fa fa-exclamation-triangle"></i>&nbsp;&nbsp;
            <span>转载规则</span>
        </p>
        
            <div class="center-align">
                <a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                    <img alt=""
                         style="border-width:0"
                         src="https://i.creativecommons.org/l/by/4.0/88x31.png"/>
                </a>
            </div>
            <br/>
            <span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text"
                  property="dct:title" rel="dct:type">
                    《损失函数CrossEntropyLoss详解》
                </span> 由
            <a xmlns:cc="http://creativecommons.org/ns#" href="/2019/08/12/代码测试/" property="cc:attributionName"
               rel="cc:attributionURL">
                Michael Don
            </a> 采用
            <a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                知识共享署名 4.0 国际许可协议
            </a>进行许可。
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>


        </div>
    </div>

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6 overflow-policy" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2019/08/12/插入图片/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/9.jpg" class="responsive-img" alt="Hexo文章中插入图片的懒人操作">
                        
                        <span class="card-title">Hexo文章中插入图片的懒人操作</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Hexo文章中插入图片的懒人操作设置_config文件在博客根目录下打开_config文件，按下图设置，这样每次新建hexo new post新建md文件时，会在同个目录下自动新建同名文件夹存放图片。

安装Typora【众所周知，Typo
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2019-08-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/博客的坑/" class="post-category" target="_blank">
                                    博客的坑
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Hexo/" target="_blank">
                        <span class="chip bg-color">Hexo</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6 overflow-policy" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2019/08/10/Task4/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/23.jpg" class="responsive-img" alt="Task4">
                        
                        <span class="card-title">Task4</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            耶斯
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2019-08-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Spider/" class="post-category" target="_blank">
                                    Spider
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/爬虫/" target="_blank">
                        <span class="chip bg-color">爬虫</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fa fa-list"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).slideUp(500);
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).slideDown(500);
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>

<footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
          &copy; 2019 Goolo. All Rights Reserved.

          
                &nbsp;<i class="fa fa-area-chart"></i>&nbsp;站点总字数:&nbsp;
                <span class="white-color">7.1k</span>
            
              
            <br>
            <span id="sitetime"></span>
              
            
			
                <br>
                
                <span id="busuanzi_container_site_pv">
                    <i class="fa fa-heart-o"></i>
                    本站总访问量 <span id="busuanzi_value_site_pv" class="white-color"></span>
                </span>
                
                
                <span id="busuanzi_container_site_uv">
                    <i class="fa fa-users"></i>
                    次,&nbsp;访客数 <span id="busuanzi_value_site_uv" class="white-color"></span> 人.
                </span>
                
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">


    <a href="mailto:1243566178@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1243566178" class="tooltipped" data-tooltip="QQ联系我: 1243566178" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>


</div>
    </div>
</footer>

<div class="progress-bar"></div>

<script language="javascript">
  function siteTime() {
  window.setTimeout("siteTime()", 1000);
  var seconds = 1000;
  var minutes = seconds * 60;
  var hours = minutes * 60;
  var days = hours * 24;
  var years = days * 365;
  var today = new Date();
  var todayYear = today.getFullYear();
  var todayMonth = today.getMonth() + 1;
  var todayDate = today.getDate();
  var todayHour = today.getHours();
  var todayMinute = today.getMinutes();
  var todaySecond = today.getSeconds();
  /* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
  year - 作为date对象的年份，为4位年份值
  month - 0-11之间的整数，做为date对象的月份
  day - 1-31之间的整数，做为date对象的天数
  hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
  minutes - 0-59之间的整数，做为date对象的分钟数
  seconds - 0-59之间的整数，做为date对象的秒数
  microseconds - 0-999之间的整数，做为date对象的毫秒数 */
  var t1 = Date.UTC(2019, 08, 09, 00, 00, 00); //北京时间2018-2-13 00:00:00
  var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
  var diff = t2 - t1;
  var diffYears = Math.floor(diff / years);
  var diffDays = Math.floor((diff / days) - diffYears * 365);
  var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
  var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) / minutes);
  var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours - diffMinutes * minutes) / seconds);
  document.getElementById("sitetime").innerHTML = "出生已经 " + diffYears + " 年 " + diffDays + " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
  }/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
  siteTime();
</script>

<!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
<!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


<script src="/libs/materialize/materialize.min.js"></script>
<script src="/libs/masonry/masonry.pkgd.min.js"></script>
<script src="/libs/aos/aos.js"></script>
<script src="/libs/scrollprogress/scrollProgress.min.js"></script>
<script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
<script src="/js/matery.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->



    <script src="/libs/others/clicklove.js"></script>


    <script async src="/libs/others/busuanzi.pure.mini.js"></script>


</body>
</html>